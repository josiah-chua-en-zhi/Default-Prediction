# Default-Prediction
This model is a stacked model of boosted trees and TabNets to predict if a person will default.

## Preprocessing and Feature Engineering

To analyse the data, the data was put into Facets a tool for ML data visualization and here are some of the key takeways that were used for feature enginenering and pre processing

### Skewed Data

![image](https://user-images.githubusercontent.com/81459293/154807825-e230e6a9-d02a-42f8-a1bd-5094d11b835d.png)

The data provided is skewed as defaulting is less common, this could lead to overfitting when trianing the model causing the model to be bad at predicting the minority class. This is detrimental for a case of predicting default as it is more inportant to be able to detect defaulters than non defaulters as the cost to the company is greater. Hence to balnce the training dataset, sampling was used.

Oversampling was prefered and used instead of undersamoling due as it was able to increase the proportion of the minority class while retaining the information form the majority class.

![image](https://user-images.githubusercontent.com/81459293/154808221-80b21154-39be-4bd6-b26b-729b722534e8.png)

SMOTE was used for oversampling. New minority class data are generated by selecting examples that are close in the feature space, and interpolating a new data point between those 2 points.

### Highly Non-Linear Numerical Data

#### Total Charges

![image](https://user-images.githubusercontent.com/81459293/154806928-23054f39-95bd-425d-b44e-1f2f5fb1f43f.png)

![image](https://user-images.githubusercontent.com/81459293/154806950-e7776d79-256e-4927-a00b-0b1dc706b2fe.png)

#### Tenure

![image](https://user-images.githubusercontent.com/81459293/154807308-1135d3e9-3db4-44e5-8403-cb1ae2791b78.png)

![image](https://user-images.githubusercontent.com/81459293/154807333-015d370e-540a-4c02-90c2-8d6d89c32934.png)

#### Monthly Charges

![image](https://user-images.githubusercontent.com/81459293/154807432-6c9e6a50-6431-4a79-8706-9b92fe409a9f.png)

![image](https://user-images.githubusercontent.com/81459293/154807453-bf7532fd-634b-4171-bfe5-5e05f119d52d.png)

As seen in the graphs for the different numerical features while tenure is slightly linear, the Mothly Charges and Total Charges are not, simply using numerical data might affect the weights for that features as it contributes to the model differently at different ranges. Hence bucketing was done where new catergorical features were engineered based on different effects that different ranges have on the percetage of people who defaulted.

## Evaluation metric

For skewed data, accuracy is not appropriate to use accuracy to evaluate the model as simply predicting the majority class will result in a good accuracy. Further more when we are especially concerned witht he minority class for defaults, it it more suitable to measure how good a model is at correctly predicting that a defaulter will default (recall), but at the same time keeping false positives low (precision). Hence the F1 Score was used as the evaluation metric.

![image](https://user-images.githubusercontent.com/81459293/154809403-dbdee3e1-c270-4320-a21f-d5df79118fa9.png)

As we can see F1 scores work better with imbalance data as it gives equal weight to Precision and Recall, hence the model cannot increase predictions of true positives at the expense of false positives.

## Model Selection

For Tabular Data most the popular choice would be to use Boosted Trees, as it is sample efficient at making decision rules from informative feature data. It is considered extremely fast, stable, faster to tune  which is well suited for tabular data. Hnece Boosted Trees often out beat Deep learning models in Data competitons.

![image](https://user-images.githubusercontent.com/81459293/154810583-8e05aa96-d2ad-4e23-9415-ecead4cf1906.png)

However, newer deep learning models such as Tabnet that have been shown to out perform boosted tress in tabular data predictions. Tabnet uses an attentive layer for features selection essentially allowing for decision boundaries, similar to boosted trees enabling interpretability and better learning as the learning capacity is used for the most useful features. Furthermore the dense layers of the feature transofmrer block also allows for futher feature generation which could allow the model to pick out new features and increase the robustness of the model.

![image](https://user-images.githubusercontent.com/81459293/154811054-b9eb7a04-a89d-4c0f-868e-4ea197c5280d.png)

Hence to maximise the cpabilites of both models, a stacked ensamble of gradient boosted trees, XGBoost and LightGBM, and TabNet which have been shown (paper in refrences) to out perfrom both models. It is done by using the prediction outputs of the individual different models and using it as inputs for a meta model like a linear regression to have a final prediction.

# How to use

## API

## Docker

# Refrences
Facets:https://pair-code.github.io/facets/
TabNet:https://github.com/titu1994/tf-TabNet
Boosted Trees and Deep learning ensamble: https://arxiv.org/pdf/2106.03253.pdf
